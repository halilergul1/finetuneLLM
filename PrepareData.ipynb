{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "transaction = pd.read_csv(\"/Users/halilergul/Desktop/thesis_researcg/data/real_data/SU_ORNEKLEM_KK_HAR_BILGI.csv\")\n",
    "demographic = pd.read_csv(\"/Users/halilergul/Desktop/thesis_researcg/data/real_data/SU_MUSTERI_KITLE_ORNEKLEM_60K.csv\")\n",
    "# make all the columns of transaction and demographic lowercase\n",
    "transaction.columns = map(str.lower, transaction.columns)\n",
    "demographic.columns = map(str.lower, demographic.columns)\n",
    "transaction.head() # ISYERI_TURU is MCC\n",
    "# read xlsx file with pandas and name it as mcc_data\n",
    "mcc_data = pd.read_excel(\"/Users/halilergul/Desktop/thesis_researcg/data/real_data/MCC.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at mcc_data where mcc is 5411 or 5691 or 5541\n",
    "mcc_data[mcc_data['mcc'].isin([5411, 5691, 5541])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join transaction with demographic on musteri_id_mask\n",
    "# But I want only the following columns from the demographic: cinsiyeti, medeni_drm_ack, egitim_drm_ack, is_turu_ack, gelir, yas\n",
    "data = pd.merge(transaction, demographic[['musteri_id_mask', 'cinsiyeti', 'medeni_drm_ack', 'egitim_drm_ack', 'is_turu_ack', 'gelir', 'yas']], on='musteri_id_mask', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum() # check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape # (1180791, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.musteri_id_mask.nunique() #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drow the rows with null values for the columns: cinsiyeti, medeni_drm_ack, egitim_drm_ack, is_turu_ack, gelir, yas, mcc, islem_tarihi, islem_tutari\n",
    "data = data.dropna(subset=['cinsiyeti', 'medeni_drm_ack', 'egitim_drm_ack', 'is_turu_ack', 'gelir', 'yas', 'isyeri_turu', 'islem_tarihi', 'islem_tutari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape) # 1123445, 17\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_data.head() # there is mcc column and it concsist of numbers like 5046. There is also column of description that explains what the mcc number means\n",
    "data.head() # there is isyeri_turu column and it concsist of numbers like 5046 so it is the same as mcc column in mcc_data\n",
    "# I want to add a column to data that is called description and it will consist of the description of the mcc number\n",
    "data = pd.merge(data, mcc_data[['mcc', 'description', 'category_name']], left_on='isyeri_turu', right_on='mcc', how='left')\n",
    "# Now these description values are like: 11 - GİYİM VE AKSESUAR. I only want the part after the dash\n",
    "data['description'] = data['description'].str.split('-').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look how many na values there are for each rows of ['cinsiyeti', 'medeni_drm_ack', 'egitim_drm_ack', 'is_turu_ack', 'gelir', 'yas', 'isyeri_turu', 'islem_tarihi', 'x', 'y', 'islem_tutari']\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twenty most frequent unqiue category names\n",
    "data['category_name'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category_name'].nunique() # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte density of data by: number of rows /number of users * number of categories\n",
    "# express in as a percentage\n",
    "data.shape[0] / data['musteri_id_mask'].nunique() * data['category_name'].nunique() / data.shape[0] * 100 # 0.0134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reducing the number of unique values of category_name column and mapping some of them to the same value:\n",
    "\n",
    "main categories will be three: Bakkallar ve Süpermarketler, Erkek ve Kadın Giyim Mağazaları, Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)\n",
    "\n",
    "Erkek ve Kadın Giyim Mağazaları(Men’s and women’s clothing stores): \n",
    "- \"Erkek ve Kadın Giyim Mağazaları\"\n",
    "- Ayakkabı Mağazaları  ???\n",
    "- Bayan Hazır Giyim Dükkanları ???\n",
    "- Erkek ve Erkek Çocuk Giysi ve Aksesuar  Dükkanları ???\n",
    "\n",
    "Bakkallar ve Süpermarketler (Grocery stores-supermarkets):\n",
    "- \"Bakkallar ve Süpermarketler\"\n",
    "- \"Çeşitli Yiyecek Dükkanları---Çok Amaçlı Dükkanlar  ve Spesiyalite Marketleri\"\n",
    "- \"Yemek Yerleri ve Restoranlar\"\n",
    "- \"Fast Food Dükkanları\"\n",
    "\n",
    "Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan) (Service stations):\n",
    "- \"Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)\"\n",
    "\"\"\"\n",
    "\n",
    "# hold categories that could not be mapped\n",
    "unmapped_categories = []\n",
    "\n",
    "def map_category_name(category):\n",
    "    global unmapped_categories  # Use the global list to keep track of unmapped categories\n",
    "    \n",
    "    # Check if category is a string\n",
    "    if not isinstance(category, str):\n",
    "        # Handle non-string category here (e.g., return a default value or the original category)\n",
    "        # Also, add to unmapped categories if it's a new, unrecognized category\n",
    "        if category not in unmapped_categories:\n",
    "            unmapped_categories.append(category)\n",
    "        return category  \n",
    "    \n",
    "    # mappings with lowercase for more flexibility\n",
    "    mappings = {\n",
    "        'erkek ve kadın giyim mağazaları': 'Erkek ve Kadın Giyim Mağazaları',\n",
    "        'bayan hazır giyim dükkanları': 'Erkek ve Kadın Giyim Mağazaları',  # Add this mapping\n",
    "        'erkek ve erkek çocuk giysi ve aksesuar dükkanları': 'Erkek ve Kadın Giyim Mağazaları',  # And this\n",
    "        'bakkallar ve süpermarketler': 'Bakkallar ve Süpermarketler',\n",
    "        'çeşitli yiyecek dükkanları---çok amaçlı dükkanlar ve spesiyalite marketleri': 'Bakkallar ve Süpermarketler',\n",
    "        'servis istasyonları (asistans-yardım servisi olan veya olmayan)': 'Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)',\n",
    "        'servis i̇stasyonları (asistans-yardım servisi olan veya olmayan)': 'Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)',\n",
    "        'bakkallar   ve süpermarketler': 'Bakkallar ve Süpermarketler',\n",
    "        'fırınlar': 'Bakkallar ve Süpermarketler',\n",
    "        'bakkallar   ve süpermarketler': 'Bakkallar ve Süpermarketler',\n",
    "        'çeşitli yiyecek dükkanları---çok amaçlı dükkanlar  ve spesiyalite marketleri': 'Bakkallar ve Süpermarketler',\n",
    "        'bayan aksesuarları ve özel giyim mağazaları': 'Erkek ve Kadın Giyim Mağazaları',\n",
    "        'erkek ve erkek çocuk giysi ve aksesuar  dükkanları': 'Erkek ve Kadın Giyim Mağazaları',\n",
    "        'aile giyim mağazaları': 'Erkek ve Kadın Giyim Mağazaları',\n",
    "        'erkek, bayan ve çocuk üniformaları ve ticari giysiler': 'Erkek ve Kadın Giyim Mağazaları'\n",
    "    }\n",
    "    \n",
    "    # Normalize the category name to lowercase to match the mappings\n",
    "    category_normalized = category.lower().strip()\n",
    "    \n",
    "    # Check and map the category, if not found add to unmapped categories list\n",
    "    mapped_category = mappings.get(category_normalized, None)\n",
    "    if mapped_category is None:\n",
    "        # If this is a new, unrecognized category, add it to the list\n",
    "        if category_normalized not in unmapped_categories:\n",
    "            unmapped_categories.append(category_normalized)\n",
    "        return category  # Return the original category if not found\n",
    "    else:\n",
    "        return mapped_category\n",
    "\n",
    "# Apply the mapping function to the category_name column\n",
    "# Assuming 'data' is a pandas DataFrame and 'category_name' is a column in it\n",
    "data['new_category_name'] = data['category_name'].apply(map_category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmapped_categories\n",
    "# I will print specific unmapped_categories which include the string erkek or kadın or giyim or bakkal or asistans or servis or süper or yiyecek\n",
    "for i in unmapped_categories:\n",
    "    # go over iterations where i is string not none or nan\n",
    "    if isinstance(i, str):\n",
    "        # if i includes the string erkek or kadın or giyim or bakkal or asistans or servis or süper or yiyecek\n",
    "        if 'erkek' in i or 'kadın' in i or 'giyim' in i or 'bakkal' in i or 'asistans' in i or 'servis' in i or 'süper' in i or 'yiyecek' in i or 'bayan' in i:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main three categories\n",
    "main_categories = ['Bakkallar ve Süpermarketler', 'Erkek ve Kadın Giyim Mağazaları', 'Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)']\n",
    "\n",
    "# Dictionary of translations\n",
    "translations = {\n",
    "    'Bakkallar ve Süpermarketler': 'Grocery',\n",
    "    'Erkek ve Kadın Giyim Mağazaları': 'Clothing',\n",
    "    'Servis İstasyonları (Asistans-Yardım Servisi Olan veya Olmayan)': 'Gas stations'\n",
    "}\n",
    "# Replace the values in the DataFrame and set others to \"Other\"\n",
    "data['new_category_name_eng'] = data['new_category_name'].map(translations).fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.new_category_name_eng.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at total number of rows where new_category_name is not in main_categories\n",
    "data[~data['new_category_name'].isin(main_categories)].shape[0] # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.new_category_name_eng.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.musteri_id_mask.nunique() # 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to filter out this data to include only customers who made 10 transactions at least and\n",
    "# I want to only include those who made a purchase from at least two categories (new_category_name_eng) \n",
    "#in their last 10 transactions (u can look at islem_tarihi to sort based on their last 10 transactions).\n",
    "# make islem_tarihi column as valid date\n",
    "data['islem_tarihi'] = pd.to_datetime(data['islem_tarihi'], format='%Y-%m-%d')\n",
    "\n",
    "# Step 1: Filter customers with at least 10 transactions\n",
    "customer_counts = data['musteri_id_mask'].value_counts()\n",
    "customers_10_plus = customer_counts[customer_counts >= 10].index\n",
    "filtered_data = data[data['musteri_id_mask'].isin(customers_10_plus)]\n",
    "\n",
    "# Step 2: Sort transactions by `islem_tarihi` and filter the last 10 transactions for each customer\n",
    "filtered_data = filtered_data.sort_values(by=['musteri_id_mask', 'islem_tarihi'])\n",
    "last_10_transactions = filtered_data.groupby('musteri_id_mask').tail(10)\n",
    "\n",
    "# Step 3: Include only those customers who made purchases from at least two different categories in their last 10 transactions\n",
    "def has_multiple_categories(group):\n",
    "    return group['new_category_name_eng'].nunique() >= 2\n",
    "\n",
    "valid_customers = last_10_transactions.groupby('musteri_id_mask').filter(has_multiple_categories)['musteri_id_mask'].unique()\n",
    "data = last_10_transactions[last_10_transactions['musteri_id_mask'].isin(valid_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.musteri_id_mask.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.new_category_name_eng.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot for the column description to see distribution of the values\n",
    "import matplotlib.pyplot as plt\n",
    "print(data['new_category_name_eng'].value_counts())\n",
    "data['new_category_name_eng'].value_counts().plot(kind='bar')\n",
    "# laso print how much percentage each category has among all the data\n",
    "print(data['new_category_name_eng'].value_counts() / data['new_category_name_eng'].value_counts().sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get those customers who appears more than 5 rows\n",
    "data.musteri_id_mask.value_counts()[data.musteri_id_mask.value_counts() > 5]\n",
    "# get average number of transactions per customer\n",
    "data.musteri_id_mask.value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gelir.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average islem_tutari for all transactions\n",
    "data.islem_tutari.mean()\n",
    "\n",
    "# get average islem_tutari per customer\n",
    "data.groupby(\"musteri_id_mask\").islem_tutari.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where row value of mcc is nan\n",
    "data = data[~data.mcc.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column as income group based on gelir column which is numeric. evenly divide it into 3 groups: low, medium, high\n",
    "data[\"income_group\"] = pd.qcut(data.gelir, 3, labels=[\"low\", \"middle\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape # (611785, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'description' is the column with the target classes\n",
    "class_counts = data['description'].value_counts()\n",
    "\n",
    "# Find the number of samples in the smallest class\n",
    "min_class_count = class_counts.min()\n",
    "\n",
    "# Resample each class to have the same number of samples as the smallest class\n",
    "data_undersampled = pd.DataFrame()\n",
    "\n",
    "for class_index in class_counts.index:\n",
    "    class_subset = data[data['description'] == class_index]\n",
    "    class_subset_undersampled = resample(class_subset, \n",
    "                                         replace=False,    # sample without replacement\n",
    "                                         n_samples=min_class_count,     # to match minority class\n",
    "                                         random_state=123) # reproducible results\n",
    "    data_undersampled = pd.concat([data_undersampled, class_subset_undersampled], axis=0)\n",
    "\n",
    "# Shuffle the order of the undersampled dataset so that it's not grouped by class\n",
    "data_undersampled = data_undersampled.sample(frac=1, random_state=123).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a small sample of data\n",
    "sample_data = data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data with those musteri_id_mask who has more than 6 or 7 transactions and has diverse mcc values at least 3\n",
    "filtered_data = data.groupby('musteri_id_mask').filter(lambda x: (len(x) > 9) and (x['mcc'].nunique() >= 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape # (467917, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.musteri_id_mask.nunique() # 8653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique combinations of x and y values are there\n",
    "filtered_data.groupby(['x', 'y']).size().reset_index().rename(columns={0:'count'}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominatim \n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import time\n",
    "# Create a geolocator with a unique user-agent\n",
    "geolocator = Nominatim(user_agent=\"your_unique_user_agent\")\n",
    "\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "        address = location.raw['address']\n",
    "        # Get only the city or the closest relevant field\n",
    "        city = address.get('city', '') or address.get('town', '') or address.get('village', '') or address.get('state', '')\n",
    "        return city\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Timed out\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Apply the function with a delay to avoid rate limiting\n",
    "for index, row in sample_data.iterrows():\n",
    "    sample_data.at[index, 'location'] = reverse_geocode(row['x'], row['y'])\n",
    "    time.sleep(1)  # Delay of 1 second between requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to perform reverse geocoding\n",
    "def reverse_geocode(lat, lon, api_key):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\n",
    "        \"latlng\": f\"{lat},{lon}\",\n",
    "        \"key\": api_key\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get('results', [])\n",
    "        if results:\n",
    "            address_components = results[0]['address_components']\n",
    "            district = next((comp['long_name'] for comp in address_components if 'administrative_area_level_2' in comp['types']), \"Unknown District\")\n",
    "            city = next((comp['long_name'] for comp in address_components if 'administrative_area_level_1' in comp['types']), \"Unknown City\")\n",
    "            return f\"{district}/{city}\"\n",
    "        else:\n",
    "            return \"No Results Found\"\n",
    "    else:\n",
    "        return \"Error in API Call\"\n",
    "\n",
    "# Extract unique lat-long combinations\n",
    "unique_lat_long = filtered_data[['x', 'y']].drop_duplicates()\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyCN1IBT0eZSq5yI2d-95x7O6SJOsMliQks\"\n",
    "\n",
    "# Reverse geocode unique combinations\n",
    "unique_lat_long['location'] = unique_lat_long.apply(lambda row: reverse_geocode(row['x'], row['y'], api_key), axis=1)\n",
    "\n",
    "# Save results\n",
    "unique_lat_long.to_csv(\"unique_lat_long.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unique_lat_long.csv file\n",
    "unique_lat_long = pd.read_csv(\"unique_lat_long.csv\")\n",
    "\n",
    "# Merge results so that in final_data I will have location column based on x and y column values that coreespond to unique_lat_long.csv file\n",
    "final_data = pd.merge(filtered_data, unique_lat_long, on=['x', 'y'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save final_data as csv file with name data_with_location.csv\n",
    "final_data.to_csv(\"/Users/halilergul/Desktop/thesis_researcg/data/real_data/data_with_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as csv\n",
    "# data.to_csv(\"data_with_street_level_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the mappings with consistent case and characters\n",
    "gender_mapping = {'E': 'male', 'K': 'female'}\n",
    "marital_status_mapping = {k.lower().strip(): v for k, v in {\n",
    "    'evli': 'married', 'bekar': 'single', 'bosanmis': 'divorced', 'dul': 'widowed', \n",
    "    'bilinmiyor': 'unknown', 'BILINMIYOR': 'unknown'\n",
    "}.items()}\n",
    "education_mapping = {k.lower(): v for k, v in {\n",
    "    'LİSE': 'high school', 'LISE': 'high school', 'ORTAOKUL': 'middle school', 'ÜNİVERSİTE': 'university', \n",
    "    'YÜKSEKOKUL': 'secondary school', 'lisansüstü': 'graduate', 'İLKOKUL': 'elementary school', \n",
    "    'bilinmiyor': 'unknown', 'ÜNIVERSITE': 'university', 'DOKTORA': 'phd', 'ilkokul': 'elementary school', \n",
    "    'egitimsiz': 'no education'\n",
    "}.items()}\n",
    "employment_mapping = {k.lower(): v for k, v in {\n",
    "    'ücretli (özel)': 'private employee', 'serbest meslek' : 'self employed','çalisan emekli(serbest meslek)': 'self employed', \n",
    "    'emekli': 'retired', 'ÜCRETLI (KAMU)': 'public employee', 'ÇALIŞMIYOR': 'unemployed', \n",
    "    'EV HANIMI': 'housewife', 'diger': 'other', 'bilinmiyor': 'unknown', \n",
    "    'ÇALIŞAN EMEKLİ(ÜCRETSİZ)': 'working retired', 'çalisan emekli(ücretli)': 'working retired', \n",
    "    'ÇALIŞAN EMEKLİ(ÜCRETLİ)': 'working retired', 'çalışan emekli(ücretsiz)': 'working retired', \n",
    "    'ögrenci': 'student', 'tanimsiz': 'unknown', 'çalismiyor': 'not working'\n",
    "}.items()}\n",
    "\n",
    "# Standardize and replace values in DataFrame\n",
    "data['cinsiyeti'] = data['cinsiyeti'].str.upper().replace(gender_mapping)\n",
    "data['medeni_drm_ack'] = data['medeni_drm_ack'].str.lower().str.strip().replace(marital_status_mapping)\n",
    "data['egitim_drm_ack'] = data['egitim_drm_ack'].str.lower().replace(education_mapping)\n",
    "data['is_turu_ack'] = data['is_turu_ack'].str.lower().replace(employment_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.egitim_drm_ack.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.is_turu_ack.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.medeni_drm_ack.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.egitim_drm_ack.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.new_category_name_eng.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_list = [\"Based on my demographic details and historical transaction data provided below, predict my next purchase category.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data as csv with name data_original\n",
    "data.to_csv(\"data_original.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.musteri_id_mask.nunique() # 8957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column islem_tarihi as datetime\n",
    "data['islem_tarihi'] = pd.to_datetime(data['islem_tarihi'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data is your DataFrame and instructions_list is defined somewhere in your code\n",
    "\n",
    "# Calculate the average total money spent in the last 15 transactions across all customers\n",
    "total_spent_per_customer = data.groupby('musteri_id_mask')['islem_tutari'].apply(lambda x: x.sort_values(ascending=False).head(15).sum())\n",
    "average_total_spent_last_15 = total_spent_per_customer.mean()\n",
    "\n",
    "json_data = []\n",
    "\n",
    "for customer_id, group in data.groupby('musteri_id_mask'):\n",
    "    # Focus only on the last 15 transactions for each customer\n",
    "    customer_data = group.sort_values('islem_tarihi', ascending=True).tail(15)\n",
    "    \n",
    "    # Ensure there are at least 15 transactions to consider\n",
    "    if len(customer_data) < 15:\n",
    "       continue\n",
    "\n",
    "    # Check for at least two unique MCC categories within these transactions\n",
    "    if len(customer_data['new_category_name_eng'].unique()) < 2:\n",
    "        continue\n",
    "\n",
    "    customer_info = customer_data.iloc[-2]  # The 14th transaction is now the second last after sorting by date ascending\n",
    "    \n",
    "    employment_info = \"I am not currently working.\" if customer_info['is_turu_ack'] == 'not working' else f\"I am working as a {customer_info['is_turu_ack']}.\"\n",
    "    if customer_info['is_turu_ack'] == 'retired':\n",
    "        employment_info = \"I am retired.\"\n",
    "    if customer_info['is_turu_ack'] == 'unknown':\n",
    "        employment_info = \"\"\n",
    "\n",
    "    education_info = \"\" if customer_info['egitim_drm_ack'] == 'unknown' else f\", {customer_info['egitim_drm_ack']} graduate\"\n",
    "    marital_status_info = \"\" if customer_info['medeni_drm_ack'] == 'unknown' else f\"{customer_info['medeni_drm_ack']} \"\n",
    "\n",
    "    # Processing MCC categories, transaction dates, and amounts for the first 14 transactions for the input string\n",
    "    mcc_list_str = ', '.join(f\"<{mcc}>\" for mcc in customer_data.head(14)['new_category_name_eng'])\n",
    "    transaction_dates_str = ', '.join(customer_data.head(14)['islem_tarihi'].dt.strftime('%Y-%m-%d'))\n",
    "    transaction_amounts_str = ', '.join(f\"${amount:.2f}\" for amount in customer_data.head(14)['islem_tutari'])\n",
    "\n",
    "    total_spent_last_14 = round(customer_data.head(14)['islem_tutari'].sum(), 2)\n",
    "\n",
    "    input_string = (\n",
    "        f\"I am <{customer_id}>. I am {customer_info['yas']} years old, \"\n",
    "        f\"{marital_status_info}{customer_info['cinsiyeti']}\"\n",
    "        f\"{education_info}, and {employment_info} \"\n",
    "        f\"In terms of my income state, I belong to the {customer_info['income_group']} income group. \"\n",
    "        f\"Recently, I made 14 transactions. \"\n",
    "        f\"In these transactions, I have spent a total of ${total_spent_last_14} dollars. \"\n",
    "        f\"I bought items from the following categories, chronologically: {mcc_list_str}. \"\n",
    "        f\"I bought from these categories on the following dates, chronologically: {transaction_dates_str}. \"\n",
    "        f\"I spent the following money for these items, chronologically: {transaction_amounts_str}. \"\n",
    "    )\n",
    "\n",
    "    response = f\"<{customer_data.iloc[-1]['new_category_name_eng']}>.\"\n",
    "\n",
    "    json_data.append({\n",
    "        \"instruction\": np.random.choice(instructions_list),\n",
    "        \"input\": input_string,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "# Save to JSON file\n",
    "with open('last_15_transactions_others.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(json_data, file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in final data, I want to check how many unique mcc values are there for each customer in their first 7 transactions, chronologically based on islem_tarihi column\n",
    "chron_df = final_data.sort_values('islem_tarihi').groupby('musteri_id_mask').head(7)\n",
    "# now filter out those customers who have less than 3 unique mcc values in their first 7 transactions\n",
    "chron_df = chron_df.groupby('musteri_id_mask').filter(lambda x: x['description'].nunique() >= 3)\n",
    "# now for each customer, get a number of unique mcc values in their first 7 transactions\n",
    "chron_df.groupby('musteri_id_mask').description.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for all info regarding musteri_id_mask 17320700\n",
    "data[data.musteri_id_mask == 24604528].sort_values(\"islem_tarihi\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot for new_category_name_eng column with high quality of dpi 300. Export it as pdf file and sort it based on the values descending\n",
    "# plot should be vertical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "sns.countplot(y='new_category_name_eng', data=data, order=data['new_category_name_eng'].value_counts().index)\n",
    "plt.savefig(\"category_count_plot.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of the values of new_category_name_eng column with normalized values\n",
    "data['new_category_name_eng'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_v2.musteri_id_mask.unique())[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the last 10 transactions of the customer with musteri_id_mask 1570895\n",
    "data['islem_tarihi'] = pd.to_datetime(data['islem_tarihi'], format='%Y-%m-%d')\n",
    "data[data.musteri_id_mask == 2594262].sort_values(\"islem_tarihi\").tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make islem_tarihi column as valid date\n",
    "data_v2['islem_tarihi'] = pd.to_datetime(data_v2['islem_tarihi'], format='%Y-%m-%d')\n",
    "data_v2[data_v2.musteri_id_mask == 2594262].sort_values(\"islem_tarihi\").tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
